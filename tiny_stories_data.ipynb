{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPRif5360ylw",
        "outputId": "91333484-07a2-4a95-bef4-7a124a8149fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset parquet (C:/Users/scott/.cache/huggingface/datasets/roneneldan___parquet/roneneldan--TinyStories-6ac769f186d7da53/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "767b886c8f114952b252ae4c32a6a198",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import tiktoken\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JlS0grS2KR_",
        "outputId": "6146c860-87eb-4cbb-9eb3-117f6d1162bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}\n",
            "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "\n",
            "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
            "\n",
            "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
          ]
        }
      ],
      "source": [
        "# print(dataset)\n",
        "# print(dataset.keys())\n",
        "encoder = tiktoken.get_encoding(\"p50k_base\")\n",
        "train_dset = dataset['train']\n",
        "print(train_dset[0])\n",
        "# print(len(train_dset))\n",
        "print(train_dset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "H8Q3mjka4Kyo"
      },
      "outputs": [],
      "source": [
        "def integer_encode(texts):\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "    encoded_texts = []\n",
        "\n",
        "    # Assign a unique integer to each word\n",
        "    current_index = 1  # Start indexing from 1, leaving 0 for padding if needed\n",
        "    for i in tqdm(range(int(len(texts)/8))):\n",
        "        text = train_dset[i]['text']\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = text.replace('\\t', ' ')\n",
        "        text = ' '.join(text.split())\n",
        "        text = re.sub(r'([.,!?;:\"()])', r' \\1', text)\n",
        "        encoded_text = []\n",
        "        for word in text.split():\n",
        "            if word not in word_to_index:\n",
        "                word_to_index[word] = current_index\n",
        "                index_to_word[current_index] = word\n",
        "                current_index += 1\n",
        "            encoded_text.append(word_to_index[word])\n",
        "        encoded_texts.append(encoded_text)\n",
        "\n",
        "    return word_to_index, index_to_word, encoded_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIPrC0BP2vPD",
        "outputId": "3ce2d137-d1fe-40b8-b5d1-4f8549480f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 264964/264964 [00:43<00:00, 6063.39it/s]\n"
          ]
        }
      ],
      "source": [
        "w_to_i, i_to_w, encoded_train = integer_encode(train_dset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr9zGDV53hX9",
        "outputId": "56c8e786-c384-47b1-8fe2-edba20ebdfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 17, 23, 17, 18, 24, 14, 8, 25, 20, 26, 27, 10, 22, 12, 28, 3, 29, 30, 31, 32, 4, 33, 34, 12, 35, 14, 8, 36, 20, 12, 28, 37, 38, 3, 39, 3, 40, 9, 41, 10, 14, 42, 43, 26, 17, 22, 44, 37, 32, 45, 35, 46, 47, 48, 28, 49, 37, 38, 3, 50, 3, 8, 3, 51, 52, 26, 27, 10, 37, 53, 54, 35, 14, 47, 55, 3, 56, 57, 27, 10, 37, 58, 27, 33, 34, 59, 35, 14, 60, 18, 61, 19, 62, 63, 23, 56, 64, 65, 37, 66, 67, 68, 14, 69, 56, 70, 3, 8, 71, 12, 28, 62, 65, 27, 10, 37, 72, 12, 35, 14, 73, 74, 75, 76, 23, 56, 77, 57, 37, 78, 79, 14]\n",
            "41733\n"
          ]
        }
      ],
      "source": [
        "print(encoded_train[0])\n",
        "print(len(w_to_i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "iXNkoluh5D-y"
      },
      "outputs": [],
      "source": [
        "# torch and torchvision imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "# device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZECqPyDj6p9e",
        "outputId": "4bb73efc-8df8-47a2-b737-996b61bdbe5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 33120/33120 [00:00<00:00, 56372.77it/s]\n"
          ]
        }
      ],
      "source": [
        "tran_dataset_input = []\n",
        "tran_dataset_label = []\n",
        "\n",
        "for i in tqdm(range(int(len(encoded_train)/8))):\n",
        "  try:\n",
        "    tran_dataset_input.append(np.array(encoded_train[i][:100]))\n",
        "    tran_dataset_label.append(np.array(encoded_train[i][1:101]))\n",
        "  except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLbdcXMR7NpD",
        "outputId": "df40585e-fcea-4c0f-e684-28d18086981e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "33120\n",
            "100\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(len(tran_dataset_input[0]))\n",
        "print(len(tran_dataset_input))\n",
        "print(len(tran_dataset_label[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "j6pz6t5o7Qoz"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + (self.pe[:x.size(0)]).squeeze()\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_feature_dim, d_model, nhead, nlayers, embedding_size):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(embedding_size, d_model)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model = d_model, nhead = nhead)\n",
        "        self.transformer = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.linear = nn.Linear(d_model, input_feature_dim)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "    def generate_mask(self, sz):\n",
        "      mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "      return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            src: Tensor, shape ``[seq_len, batch_size]``\n",
        "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        mask = self.generate_mask(len(x)).to(device)\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DwH6U1ED8mrb"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "embed_len = 150\n",
        "vocab_size = 300\n",
        "heads = 8\n",
        "layers = 6\n",
        "hidden_size = 3*vocab_size\n",
        "input_feature_dim = 100\n",
        "tran = TransformerModel(input_feature_dim, hidden_size-(hidden_size % heads), nhead = heads, nlayers = layers,  embedding_size = embed_len).to(device)\n",
        "# print(hidden_size-(hidden_size % heads))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(tran.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0FQHjrdD9KfI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\scott\\OneDrive\\Documents\\GitHub\\ESE546Project\\tiny_stories_data.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m current_labels \u001b[39m=\u001b[39m current_labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output,current_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# except:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#   break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/scott/OneDrive/Documents/GitHub/ESE546Project/tiny_stories_data.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\scott\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\scott\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\scott\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mcross_entropy(\u001b[39minput\u001b[39m, target, weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_index, reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_smoothing)\n",
            "File \u001b[1;32mc:\\Users\\scott\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mcross_entropy_loss(\u001b[39minput\u001b[39m, target, weight, _Reduction\u001b[39m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
          ]
        }
      ],
      "source": [
        "tran_train_l = []\n",
        "tran_train_e = []\n",
        "tran_val_l = []\n",
        "tran_val_e = []\n",
        "\n",
        "num_epochs = 1\n",
        "i = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for sequence in tran_dataset_input:\n",
        "        print(i)\n",
        "        tran.zero_grad()\n",
        "\n",
        "        sequence = torch.from_numpy(sequence.T).long()\n",
        "        sequence = sequence.to(device)\n",
        "        # print(sequence.shape)\n",
        "        output = tran(sequence)\n",
        "\n",
        "        current_labels = torch.from_numpy(tran_dataset_label[i])\n",
        "        current_labels = current_labels.to(device)\n",
        "        # try:\n",
        "        loss = criterion(output,current_labels)\n",
        "        # except:\n",
        "        #   break\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if (i % 1000) != 0:\n",
        "            optimizer.step()\n",
        "\n",
        "        tran_train_l.append(loss.item())\n",
        "        predicted_labels = torch.argmax(output,dim=1)\n",
        "        train_error = 1.0 - (current_labels == predicted_labels).sum()/(len(sequence)-1)\n",
        "        tran_train_e.append(train_error.item())\n",
        "        i += 1\n",
        "\n",
        "        # if (i % 1000) == 0:\n",
        "        #     # Validation\n",
        "        #     val_sequence = tran_dataset_input[i+1]\n",
        "        #     val_sequence = torch.from_numpy(val_sequence).long().to(device)\n",
        "        #     val_output = tran(val_sequence)\n",
        "        #     val_labels = torch.from_numpy(tran_dataset_label[i+1]).to(device)\n",
        "        #     val_loss = criterion(val_output, val_labels)\n",
        "        #     tran_val_l.append(val_loss.item())\n",
        "        #     future_predicted_labels = torch.argmax(val_output, dim=1)\n",
        "        #     validation_error = 1.0 - (val_labels == future_predicted_labels).sum() / (len(val_sequence) - 1)\n",
        "        #     print(f'training_error: {train_error}')\n",
        "        #     print(f'validation_error: {validation_error}')\n",
        "        #     tran_val_e.append(validation_error.item())\n",
        "        #     print(f'Currently at {(i / len(tran_dataset_input)) * 100}% complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iYNUoVB_pHx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
